system.time()
system.time(ls())
system.time(x<-1:10000000)
?system.time
URL <- "http://ichart.finance.yahoo.com/table.csv?s=SPY"
dat <- read.csv(URL)
View(dat)
rm(list =ls())
save.image("~/clean.R.RData")
source('~/Wallstreet.R')
source('~/Wallstreet.R')
source('~/Wallstreet.R')
View(dat)
dat
dat[dat$Close == dat$Adj.Close,]
n = c(2, 3, 5)
s = c("aa", "bb", "cc")
b = c(TRUE, FALSE, TRUE)
df = data.frame(n, s, b)
l = rnorm(3,1)
?rbind
df$l <- l
View(df)
dat[["Adj.Open"]] <- dat$Adj.Close * dat$Open / dat$Close
View(dat)
source('~/Wallstreet.R')
View(dat)
source('~/Wallstreet.R')
View(dat)
options(stringsAsFactors = F)
require(ggplot2)
require(gridExtra)
# Creating the environment
assign("envPrevData", new.env(), envir = .GlobalEnv)
# Function to get the data
GetLiveData <- function(sSymbol = "GOOG")
{
sAddress <- paste("http://download.finance.yahoo.com/d/quotes.csv?s=",
sSymbol, "&f=nsb2b3v0&e=.csv", sep = "")
cat("Downloading data from ", sAddress, "\n")
dfYahooData <- read.table(sAddress, sep = ",", header = F)
Time <- Sys.time()
dfYahooData <- cbind(dfYahooData, "Time" = Time)
names(dfYahooData) <- c("Name", "Symbol", "Ask", "Bid", "Volume", "Time")
dfYahooData
}
# Function to update the current data
UpdateStockData <- function(sSymbol = "GOOG")
{
envPrevData <- get("envPrevData", envir = .GlobalEnv, mode = "environment")
dfCurr <- GetLiveData(sSymbol = sSymbol)
print(dfCurr)
try(envPrevData$dfStockData <- rbind.data.frame(envPrevData$dfStockData, dfCurr))
assign("envPrevData", envPrevData, envir = .GlobalEnv)
invisible()
}
# The plot function
plotChart <- function(){
envPrevData <- get("envPrevData", envir = .GlobalEnv, mode = "environment")
dfStockData <- envPrevData$dfStockData
if(nrow(dfStockData) > 4){
AskMovement <- factor(sign(c(0, diff(dfStockData$Ask))), levels = c(-1, 0, 1),
labels = c("Down", "No Change", "Up"))
BidMovement <- factor(sign(c(0, diff(dfStockData$Bid))), levels = c(-1, 0, 1),
labels = c("Down", "No Change", "Up"))
VolumeChange <- c(0, diff(dfStockData$Volume))
dfStockData <- data.frame(dfStockData, AskMovement, BidMovement, VolumeChange)
dfStockData$Mid <- with(dfStockData, .5*(Bid + Ask))
bAPlot <- ggplot(dfStockData, aes(Time, Mid,
ymin = Bid, ymax= Ask, colour = AskMovement))
bAPlot <- bAPlot + geom_linerange(lwd = 1.5) + xlab("") + ylab("Price\n")
bAPlot <- bAPlot +  theme(legend.position = "top", plot.margin = unit(c(0, .5, -1.5, 0), "lines"),
axis.text.y = element_text(angle = 90), axis.text.x = element_blank()) +
labs(colour = "Ask Movement") +
xlim(range(dfStockData$Time)) + scale_colour_manual(values=c("red", "blue", "green"))
VolPlot <- qplot(y = VolumeChange, x = Time, data=dfStockData, geom="bar",
stat = "identity", fill = AskMovement)
VolPlot <- VolPlot + theme(legend.position = "none", plot.margin = unit(c(0, .5, 0, 0), "lines"),
axis.text.y = element_text(angle = 90)) + xlab("\nTime") + ylab("Volume Change\n") +
xlim(range(dfStockData$Time)) + scale_colour_manual(values=c("red", "blue", "green"))
grid.arrange(bAPlot, VolPlot, nrow = 2, heights = c(1.5, 1))
}
}
# Running the process
CurrTime <- Sys.time()
while(Sys.time() < CurrTime + 60*60){
UpdateStockData("GOOG")
plotChart()
Sys.sleep(30)
}
source('~/Wallstreet.R')
plot(dat)
source('~/Wallstreet.R')
View(dat)
r <- 1:10
cx <- cumsum(x)
ma <- function(arr, n=15){
res = arr
for(i in n:length(arr)){
res[i] = mean(arr[(i-n):i])
}
res
}
source('~/Wallstreet.R')
ma(1:100)
rm(list = ls())
getwd()
cd <- read.table("Baltimore_Fixed_Speed_Cameras.csv")
head(cd)
cd <- read.table("Baltimore_Fixed_Speed_Cameras.csv", header = TRUE)
cd <- read.table("Baltimore_Fixed_Speed_Cameras.csv", header = TRUE, sep = ",")
View(cd)
cd <- read.csv("Baltimore_Fixed_Speed_Cameras.csv")
?write.xlsx
??write.xlsx
write.xlsx
library(xlsx)
install.package(xlsx)
?xlsx
??xlsx
install.packages("xlsx")
library(xlsx)
library(xlsx)
install.packages("XML")
library(XML)
fu <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fu, useInternal = TRUE)
doc
rn <- xmlRoot(doc)
xmlName(rn)
rn
xmlName(rn)
names(rn)
rn$food
rn$food[]
rn$food
rn[[1]]
rn[[2]]
rn[[3]]
rn[[4]]
rn[[5]]
rn[[5]][[1]]
xmlSApply(rn,xmlValue)
install.packages("jsonlite")
library(jsonlite)
jd <- fromJSON("https://api.github.com/users/jtleek/repos")
class(jd)
names(jd)
jd$name
names(jd$owner)
class(jd$owner)
class(jd$names)
class(jd$name)
myjson <- toJSON(iris, pretty=TRUE)
myjson
url <- 'hola'
url
url <- "hola"
url
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(url,'mifile.csv')
a <- download.file(url,'mifile.csv')
a
mydata = read.csv("mifile.csv")
mydata
heqad (mydata)
head (mydata)
names(mydata)
mydata(VAL)
mydata("VAL")
cat(mydata)
STR(mydata)
str(mydata)
mydata($VAL)
subset(mydata, VAL > 1000000)
names(mydata)
mydata[,37]
mydata[,37=24]
mydata[,37]
ndf <- mydata[,37]
names(ndf)
ndf
ndf[ndf = 24]
ndf[ndf = "24"]
ndf[ndf = 6]
ndf[15]
mydata[15]
mydata[15,VAL]
mydata[15,$VAL]
mydata[15,mydata$VAL]
names(mydata)
mydata[15,37]
mydata[,37]
X <- mydata[,37]
x
x <- mydata[,37]
x
x[x > 6]
x[x > 12]
x[x > 21]
x[x > 23]
x[x == 23]
x[x == 24]
x[x == 24]
x[x == 24]
y <- x[x == 24]
y
bad <- is.na(y)
bad
y[bad]
y[!bad]
URL <- 'https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx'
download.file(url,destfile='excel.xlsx')
download.file(URL,destfile='excel.xlsx')
LIBRARY(XLSX)
library(xlsx)
dat <- read.xlsx('excel.xlsx')
dat <- read.xlsx('excel.xlsx',1)
dat <- read.xlsx('excel.xlsx',sheetIndex=1)
dat <- read.xlsx('excel.xlsx', sheetIndex=1, header=TRUE)
URL
download.file(URL, destfile='dat.xlsx')
download.file(URL, destfile='dat3.xlsx', method="curl")
download.file(URL, destfile='dat3.xlsx', method="wget")
download.file(URL, destfile='dat3.xlsx', method="auto")
dat <- read.xlsx('dat2.xlsx', sheetIndex=1, header=TRUE)
ri <- 18:23
ci <- 7:15
dat <- read.xlsx('dat2.xlsx', sheetIndex=1, colIndex = ci, rowIndex = ri, header = TRUE)
dat
sum(dat$Zip*dat$Ext,na.rm=T)
url <- 'https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml'
doc <- xmlTreeParse(url, useInternal=TRUE)
library(XML)
doc <- xmlTreeParse(url, useInternal=TRUE)
url <- 'https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml'
url
doc <- xmlTreeParse(url, useInternal=TRUE)
doc <- xmlTreeParse(url)
doc <- htmlTreeParse(url)
url <- 'http://www.w3schools.com/xml/simple.xml'
doc <- xmlTreeParse(url, useInternal=TRUE)
cat(doc)
doc
root <- xmlRoot(doc)
root
xmlName(root)
names(root)
root[[1]]
root[[1]][[1]]
root[[1]][[2]]
xmlSApply(root, xmlValue)
xpathSApply(root, "//name", xmlValue)
xpathSApply(root, "//price", xmlValue)
xpathSApply(root, "//dino", xmlValue)
url <- 'https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml'
doc <- xmlTreeParse(url, useInternal=TRUE)
url <- 'http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml'
doc <- xmlTreeParse(url, useInternal=TRUE)
doc
root <- xmlRoot(doc)
root
xmlName(root)
names(root)
root
xpathSApply(root, "//zipcode", xmlValue)
zc <- xpathSApply(root, "//zipcode", xmlValue)
zc
zc[zc == 21231]
count(zc[zc == 21231])
zc[zc == 21231]
table(zc[zc == 21231])
library(data.table)
fread('https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv')
fread('http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv')
DT <- fread('http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv')
str(DT)
tapply(DT$pwgtp15,DT$SEX,mean)
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
system.time(replicate(100, sqrt(seq(1.0, 1.0e6))))
system.time(replicate(100, sqrt(seq(1.0, 1.0e6))))
tapply(DT$pwgtp15,DT$SEX,mean)
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
system.time(DT[,mean(pwgtp15),by=SEX])
system.time(mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15))
mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15)
system.time(mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15))
system.time(DT[,mean(pwgtp15),by=SEX])
user.time(DT[,mean(pwgtp15),by=SEX])
time(DT[,mean(pwgtp15),by=SEX])
DT[,mean(pwgtp15),by=SEX]
system.time(DT[,mean(pwgtp15),by=SEX])
unix.time(DT[,mean(pwgtp15),by=SEX])
proc.time(DT[,mean(pwgtp15),by=SEX])
system.time( for(i in 1:10) DT[,mean(pwgtp15),by=SEX])
system.time( for(i in 1:100) DT[,mean(pwgtp15),by=SEX])
system.time( for(i in 1:1000) DT[,mean(pwgtp15),by=SEX])
system.time( for(i in 1:1000) tapply(DT$pwgtp15,DT$SEX,mean) )
system.time( for(i in 1:1000) DT[,mean(pwgtp15),by=SEX] )
system.time( for(i in 1:1000) mean(DT[DT$SEX==1,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15) )
system.time( for(i in 1:1000) mean(DT[DT$SEX==1,]$pwgtp15) )
system.time( for(i in 1:1000) mean(DT[DT$SEX==1,]$pwgtp15) )
system.time( for(i in 1:1000) mean(DT[DT$SEX==2,]$pwgtp15) )
system.time( for(i in 1:1000) rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2] )
system.time( for(i in 1:1000) rowMeans(DT)[DT$SEX==1] )
rowMeans(DT)[DT$SEX==1]
DT[DT$SEX==1]
rowMeans(DT)[DT$SEX==1]
rowMeans(DT[DT$SEX==1])
rowMeans
DT[DT$SEX==1])
DT[DT$SEX==1]
system.time ( for (i in 1:1000) mean(DT$pwgtp15,by=DT$SEX) )
system.time ( for (i in 1:1000) sapply(split(DT$pwgtp15,DT$SEX),mean) )
system.time ( for (i in 1:1000) tapply(DT$pwgtp15,DT$SEX,mean) )
system.time ( for (i in 1:1000) DT[,mean(pwgtp15),by=SEX] )
system.time ( for (i in 1:1000) mean(DT[DT$SEX==1,]$pwgtp15) )
system.time ( for (i in 1:1000) mean(DT[DT$SEX==2,]$pwgtp15) )
rowMeans(DT)[DT$SEX==1]
mean(DT$pwgtp15,by=DT$SEX)
system.time ( for (i in 1:1000) mean(DT$pwgtp15,by=DT$SEX) )
system.time ( for (i in 1:1000) sapply(split(DT$pwgtp15,DT$SEX),mean) )
zc
zc[zc == 21213]
zc[zc == 21231]
thepage = readLines('http://calbears.cstv.com/sports/m-basebl/sched/cal-m-basebl-sched.html')
thepage = readLines('http://biostat.jhsph.edu/~jleek/contact.html')
thepage
thepage[180]
thepage[10]
length(thepage[10])
len(thepage[10])
nchar(thepage[10])
nchar(thepage[20])
nchar(thepage[30])
nchar(thepage[100])
a = readJPEG("http://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg")
install.packages(jpeg)
install.packages("jpeg")
a = readJPEG("http://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg")
package(jpeg)
library(jpeg)
a <- readJPEG("http://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg")
readJPEG("http://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg")
readJPEG("https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg")
readJPEG("https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg")
readJPEG("http://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg")
readJPEG("http://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg", native = FALSE)
readJPEG("http://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg", native = true)
readJPEG("http://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg", native = TRUE)
img <- readJPEG("c:\temp\jeff.jpg")
img <- readJPEG("jeff.jpg")
getwd()
img <- readJPEG("jeff.jpg")
quantile(img, probs = c(0.3, 0.8))
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg"
f <- file.path(getwd(), "jeff.jpg")
download.file(url, f, mode = "wb")
img <- readJPEG(f, native = TRUE)
quantile(img, probs = c(0.3, 0.8))
getwd()
ls()
dir()
cd gcdataproject
setwd("gcdataproject")
getwd()
dir()
getwd()
setwd("")
setwd("/")
getwd()
setwd(".")
getwd()
setwd("c:/users/dino/documents")
getwd()
dir()
train_data <- read.table("train/X_train.txt")
rm(ls())
rm(list = ls())
a <- read.table("a.txt")
View(a)
names(a)
a[1,]
a[,1]
b <- read.table("b.txt")
View(b)
rbind(a,b)
cbind(a,b)
c <- read.table("train/subject_train.txt")
View(c)
rm(list = ls())
source('~/run_analysis.R')
# The following script creates a tidy data set from some "wearable computing" data collected with accelerometers and gyroscopes of a Samsung smartphone.
#
# 4. Appropriately labels the data set with descriptive variable names.
#
# 5. From the data set in step 4, creates a second, independent tidy data set with the average of each variable for each activity and each subject.
# Note: Creo que es un GROUP BY
#
#
#
# Process Development:
# ====================
#
# 1. Merges the training and the test sets to create one data set.
# ----------------------------------------------------------------
# Load "train" data
# Data
X_train <- read.table("train/X_train.txt")
# activity ids
# y_train.txt
# subject ids
# subject_train.txt
# Load "test" data
# Data
# X_test.txt
# activity ids
# y_test.txt
# subject ids
# subject_test.txt
# Merge data
# 3. Uses descriptive activity names to name the activities in the data set
# Load "label" data
# activity labels
# 4. Appropriately labels the data set with descriptive variable names.
# feature labels
# features.txt
# feature labels
# features.txt
source('~/run_analysis.R')
source('~/run_analysis.R')
rm(list = ls())
source('~/run_analysis.R')
View(train_data)
View(train_subject_ids)
View(train_activity_ids)
rm(list = ls())
source('~/run_analysis.R')
source('~/run_analysis.R')
rm(list = ls())
source('~/run_analysis.R')
library(datasets)
data(cars)
cars
View(cars)
with(cars, plot(speed,dist))
library(lattice)
state <- data.frame(state.x77, region = state.region)
View(state)
xyplot(life.exp - income | region, data = state, layout = c(4,1) )
xyplot(Life.exp - income | region, data = state, layout = c(4,1) )
xyplot(Life.Exp - income | region, data = state, layout = c(4,1) )
xyplot(Life.Exp - Income | region, data = state, layout = c(4,1) )
getwd()
setwd("c:\\Users\\Dino\\ExData_Plotting1")
library(sqldf)
gap <- read.csv.sql("../Documents/household_power_consumption.txt", sql = "select Date, Time, Global_active_power from file where Date = '1/2/2007' or Date = '2/2/2007'", header = TRUE, sep = ";")
gap
gap$Date
as.Date(gap$Date)
View(gap)
as.Date(gap$Date,"%d")
as.Date(gap$Date,"%d/%m/%Y")
gap$Date <- as.Date(gap$Date,"%d/%m/%Y")
gap$Date
paste(gap$Date, gap$Time)
gap <- read.csv.sql("../Documents/household_power_consumption.txt", sql = "select Date, Time, Global_active_power from file where Date = '1/2/2007' or Date = '2/2/2007'", header = TRUE, sep = ";")
with(gap; paste(Date,Time))
paste(gap$Date,gap$Time)
with(gap, paste(Date,Time))
with(gap, paste(as.Date(Date,"%d/%m/%Y"),Time)
;
with(gap, paste(as.Date(Date,"%d/%m/%Y"),Time))
with(gap, plot(paste(as.Date(Date,"%d/%m/%Y"),Time), Global_active_power))
with(gap, plot(paste(as.Date(Date,"%d/%m/%Y"),Time), Global_active_power))
with(gap, paste(as.Date(Date,"%d/%m/%Y"),Time))
x_axis <- with(gap, paste(as.Date(Date,"%d/%m/%Y"),Time))
x_axis
with(gap, plot(paste(as.Date(Date,"%d/%m/%Y"),Time), Global_active_power))
x_axis <- with(gap, paste(as.Date(Date,"%d/%m/%Y"),Time))
plot(1,1)
plot(x_axis)
plot(x_axis,1)
strptime(x_axis)
strptime(x_axis[1])
x_axis <- with(gap, paste(as.Date(Date,"%d/%m/%Y"),Time))
x_axis[1]
strptime(x_axis[1],"")
strptime(x_axis[1],"%Y")
strptime(x_axis[1],"%Y-")
strptime(x_axis[1],"%Y-%m-")
strptime(x_axis[1],"%Y-%m-%d ")
strptime(x_axis[1],"%Y-%m-%d %H")
strptime(x_axis[1],"%Y-%m-%d %H:%M")
strptime(x_axis[1],"%Y-%m-%d %H:%M:%S")
x_axis <- strptime(x_axis,"%Y-%m-%d %H:%M:%S")
plot(x_axis)
plot(x_axis,1)
plot(x_axis,gap$Global_active_power)
x_axis <- with(gap, paste(as.Date(Date,"%d/%m/%Y"),Time))
plot(x_axis,gap$Global_active_power)
plot(x_axis, gap$Global_active_power, type = "l")
x_axis <- strptime(x_axis,"%Y-%m-%d %H:%M:%S")
plot(x_axis, gap$Global_active_power, type = "l")
plot(x_axis, gap$Global_active_power, type = "l", ylab = "Global Active Power (kilowatts)")
plot(x_axis, gap$Global_active_power, type = "l", ylab = "Global Active Power (kilowatts)", xlab = "")
plot(x_axis, gap$Global_active_power, type = "l", ylab = "Global Active Power (kilowatts)", xlab = "")
rm (list = ls())
dev.off()
dev.off()
dev.off()
source('C:/Users/Dino/ExData_Plotting1/plot2.R')
